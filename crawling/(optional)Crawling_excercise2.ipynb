{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"(Optional)-웹-크롤링2---Dynamic-Crawling\">(Optional) 웹 크롤링2 - Dynamic Crawling<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#(Optional)-%EC%9B%B9-%ED%81%AC%EB%A1%A4%EB%A7%812---Dynamic-Crawling\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"0.-라이브러리\">0. 라이브러리<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#0.-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"1.-Selenium-기초\">1. Selenium 기초<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#1.-Selenium-%EA%B8%B0%EC%B4%88\">¶</a></h1><p>자신의 크롬 버전을 확인하고 크롬 <a href=\"https://chromedriver.chromium.org/downloads\">웹드라이버</a>를 다운받아놓아야합니다.</p>\n",
    "<ul>\n",
    "<li>2020.09.13 기준 최신 버전: <code>85.0.4183.102</code></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.1.-Simple-Text-Crawling\">1.1. Simple Text Crawling<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#1.1.-Simple-Text-Crawling\">¶</a></h3><p>멜론 사이트에서 노래 제목을 크롤링해보자</p>\n",
    "<p>URL: <a href=\"https://www.melon.com/chart/index.htm\">https://www.melon.com/chart/index.htm</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DRIVER_PATH = 'D:/dev_files/webdrivers/chrome/chromedriver85.exe'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH) \n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# title crawling\n",
    "title = WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#frm > div > table > tbody > tr:nth-child(1) > td:nth-child(4) > div > div\")))\n",
    "\n",
    "# print(\"Title: {}\".format(title.text))\n",
    "\n",
    "title.text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>css selector의 <strong><code>규칙</code></strong>을 찾아본다</p>\n",
    "<ul>\n",
    "<li>1번째 제목: #frm &gt; div &gt; table &gt; tbody &gt; tr:nth-child(<strong>1</strong>) &gt; td:nth-child(4) &gt; div &gt; div\"</li>\n",
    "<li>2번째 제목: #frm &gt; div &gt; table &gt; tbody &gt; tr:nth-child(<strong>2</strong>) &gt; td:nth-child(4) &gt; div &gt; div</li>\n",
    "</ul>\n",
    "<p>.\n",
    ".\n",
    ".</p>\n",
    "<ul>\n",
    "<li>100번째 제목: #frm &gt; div &gt; table &gt; tbody &gt; tr:nth-child(<strong>100</strong>) &gt; td:nth-child(4) &gt; div &gt; div</li>\n",
    "</ul>\n",
    "<hr/>\n",
    "<p>또는 <code>XPATH</code>로도 확인해보자 (<code>full Xpath</code>)</p>\n",
    "<ul>\n",
    "<li>1번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[<strong>1</strong>]/td[4]/div/div</li>\n",
    "<li>2번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[<strong>2</strong>]/td[4]/div/div</li>\n",
    "</ul>\n",
    "<p>.\n",
    ".\n",
    ".</p>\n",
    "<ul>\n",
    "<li>50번째 제목: //*[@id=\"frm\"]/div/table/tbody/tr[<strong>100</strong>]/td[4]/div/div</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2번째 제목 크롤링\n",
    "WebDriverWait(driver, 20) \\\n",
    "    .until(EC.presence_of_element_located((By.XPATH, \"//*[@id='frm']/div/table/tbody/tr[2]/td[4]/div/div\"))).text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.2.-Text-Crawling-with-for-loop\">1.2. Text Crawling with for loop<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#1.2.-Text-Crawling-with-for-loop\">¶</a></h3><p>위에서 찾은 Xpath의 규칙을 바탕으로 for loop 만들자</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "\n",
    "# title crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    title = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f\"//*[@id='frm']/div/table/tbody/tr[{i}]/td[4]/div/div\")))\n",
    "    title_list.append(title.text)\n",
    "    \n",
    "print(title_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>나중에 필요한 변수(제목, 가수, 가사... 등)을 모두 긁어 한번에 <strong>데이터프레임</strong>으로 저장하여 보관한다!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.3.-Text-Crawling-(Click-&amp;-Back)\">1.3. Text Crawling (Click &amp; Back)<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#1.3.-Text-Crawling-(Click-&amp;-Back)\">¶</a></h3><p>클릭하고 나오기 -&gt; 동적 크롤링 가능 (<strong><code>가사</code></strong> 크롤링 가능)</p>\n",
    "<p>노래 제목에 링크가 걸려있기 때문에, 해당 링크까지의 <strong>XPath</strong>를 추가한다.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 1번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[1]/td[3]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()\n",
    "\n",
    "\n",
    "# 2번째 click하기\n",
    "click_element = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"frm\"]/div/table/tbody/tr[2]/td[3]/div/a')))\n",
    "click_element.click()    \n",
    "\n",
    "# back\n",
    "driver.back()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"1.4.-Text-Crawling-including-contents\">1.4. Text Crawling including contents<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#1.4.-Text-Crawling-including-contents\">¶</a></h3><ul>\n",
    "<li><p><strong>1.2</strong>처럼 for문과 함께 써보자! (첫 페이지 5개의 글에 대해 <code>title</code>, <code>artist</code>, <code>heart</code>(하트 갯수), <code>lyrics</code>(가사)를 크롤링</p>\n",
    "</li>\n",
    "<li><p><strong>1.3</strong>에서 사용한 click &amp; back을 활용하자</p>\n",
    "</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "title_list = []\n",
    "artist_list = []\n",
    "heart_list = []\n",
    "lyrics_list = []\n",
    "\n",
    "# crawling (TOP 5)\n",
    "for i in range(1, 6):\n",
    "    # click\n",
    "    click_element = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.XPATH, f'//*[@id=\"frm\"]/div/table/tbody/tr[{i}]/td[3]/div/a')))\n",
    "    click_element.click()\n",
    "\n",
    "    # title crawling\n",
    "    title = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.song_name\")))\n",
    "    title_list.append(title.text)\n",
    "\n",
    "    # artist crawling\n",
    "    artist = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#downloadfrm > div > div > div.entry > div.info > div.artist > a > span:nth-child(1)\")))\n",
    "    artist_list.append(artist.text)\n",
    "    \n",
    "    # heart crawling\n",
    "    heart = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_like_count\")))\n",
    "    heart_list.append(heart.text)\n",
    "\n",
    "    # lyrics crawling\n",
    "    lyrics = WebDriverWait(driver, 20).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \"#d_video_summary\")))\n",
    "    lyrics_list.append(lyrics.text)\n",
    "    \n",
    "    # back\n",
    "    driver.back()\n",
    "    \n",
    "print(title_list)\n",
    "print(artist_list)\n",
    "print(heart_list)\n",
    "print(lyrics_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3 id=\"TIP:-보통은-결과값을-데이터프레임-형태로-저장한다\">TIP: 보통은 결과값을 <code>데이터프레임</code> 형태로 저장한다<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#TIP:-%EB%B3%B4%ED%86%B5%EC%9D%80-%EA%B2%B0%EA%B3%BC%EA%B0%92%EC%9D%84-%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%94%84%EB%A0%88%EC%9E%84-%ED%98%95%ED%83%9C%EB%A1%9C-%EC%A0%80%EC%9E%A5%ED%95%9C%EB%8B%A4\">¶</a></h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 결과 변수\n",
    "raw_result = {'title': title_list,\n",
    "              'artist': artist_list,\n",
    "              'heart': heart_list,\n",
    "          'lyrics': lyrics_list}\n",
    "\n",
    "result = pd.DataFrame(raw_result)\n",
    "\n",
    "# # csv 파일로 save\n",
    "# result.to_csv(\"MelonTop5\", mode='w')\n",
    "\n",
    "# driver 종료\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><strong><code>데이터프레임</code></strong> 형식을 이용하면, 가독성도 좋고, 나중에 데이터 핸들링하기에도 편리하다!</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"2.-Image-Crawling\">2. Image Crawling<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#2.-Image-Crawling\">¶</a></h1><p>이미지 크롤링하기</p>\n",
    "<ul>\n",
    "<li>1번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[<strong>1</strong>]/td[4]/div/a/img</li>\n",
    "<li>2번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[<strong>2</strong>]/td[4]/div/a/img</li>\n",
    "</ul>\n",
    "<p>...</p>\n",
    "<ul>\n",
    "<li>50번째 이미지: /html/body/div/div[3]/div/div/div[4]/form/div/table/tbody/tr[<strong>50</strong>]/td[4]/div/a/img</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"STEP1.-URL-Crawling\">STEP1. URL Crawling<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#STEP1.-URL-Crawling\">¶</a></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " # chrome driver 설정\n",
    "driver = webdriver.Chrome(DRIVER_PATH)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "url = \"https://www.melon.com/chart/index.htm\"\n",
    "\n",
    "driver.get(url)\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 빈 리스트 변수\n",
    "link_list = []\n",
    "\n",
    "# # img crawling (TOP 50)\n",
    "for i in range(1, 51):\n",
    "    \n",
    "    img = WebDriverWait(driver, 20) \\\n",
    "        .until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"#frm > div > table > tbody > tr:nth-child({i}) > td:nth-child(2) > div > a > img\")))\n",
    "\n",
    "    link_list.append(img.get_attribute('src'))\n",
    "\n",
    "print(link_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h4 id=\"STEP2.-Download-images-using-URLs\">STEP2. Download images using URLs<a class=\"anchor-link\" href=\"https://eclass2.ajou.ac.kr/bbcswebdav/pid-677816-dt-content-rid-9690517_1/courses/2020U00020032020084481/Crawling_exercise2.html#STEP2.-Download-images-using-URLs\">¶</a></h4><p>자신의 디렉토리에 <code>img</code> 폴더 생성하고 실행</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib.request\n",
    "\n",
    "count = 0\n",
    "for link in link_list:\n",
    "    count += 1\n",
    "    urllib.request.urlretrieve(link, './img/img' + str(count) + '.jpg')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
